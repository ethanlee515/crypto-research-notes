# Relating KL Divergence to Cryptographic Game Hops

Here we outline our plans and efforts to formally verify a particularly tricky step in the CKKS security analysis.

## Overview

It is perhaps a common idea to replace a distribution with another similar one,
though there are variations in how this intuition is treated.
The theoretical cryptography community often measures the difference between two distributions using the total variation distance.
In our analysis of CKKS, however, the distance between distributions are taken as the KL-divergence.
We follow the analysis in ["Gaussian Sampling over the Integers: Efficient, Generic, Constant-Time"](https://ia.cr/2017/259) which uses a generalization of KL-divergence called $\lambda$*-efficient measures*.
We now state the main lemma we aim to prove.

**Lemma 3.3, rephrased**:
Let $\kappa\in\mathbb{Z}$ be the security parameter, and let $\delta$ be a $2^{-\kappa/2}$-efficient measure.
Let $\mathcal{D}(\theta)$ and $\mathcal{D}'(\theta)$ be parametrized distributions satisfying $\delta(\mathcal{D}(\theta), \mathcal{D}'(\theta))<2^{-\kappa/2}$.
Let $\mathscr{G}$ be an experiment drawing at most $q$ samples from $\mathcal{D}$ and then outputting either $\mathsf{accept}$ or $\mathsf{reject}$.
We moreover assume $\mathscr{G}$ satisfies that $\Pr[\mathsf{accept}\leftarrow\mathscr{G}]<2^{-\kappa}$.
Define $\mathscr{G}'$ to run $\mathscr{G}$ but sample from $\mathcal{D}'$ instead.
We can then bound the accepting probability of $\mathscr{G}'$ as $\Pr[\mathsf{accept}\leftarrow\mathscr{G}']<8\times 2^{-\kappa}$.

### Challenges

As we will see later, it is difficult to faithfully reproduce the lemma's proof using EasyCrypt.
For now we highlight some limitations of EasyCrypt.

#### Making Copies of an Adversary

Given an adversary in EasyCrypt, there is no way to clone and run many independent copies of it out of the box.
Unfortunately, the proof of Lemma 3.3 is one of the many proofs that requires precisely this.
The work ["Reflection, Rewinding, and Coin-Toss in EasyCrypt"](https://ia.cr/2021/1078) addresses this challenge.
It however requires adding additional *rewindable* assumptions to the adversary, and directly accessing the adversary's internal states using the `glob` feature which has historically been difficult to get right.

#### Treatment of Statistical Distance and Indistinguishability

In the analysis of cryptographic games, one often assumes without loss of generality that an adversary runs some "next message function" for each round of the interaction.
One then (sometimes implicitly) applies the data processing inequality to this function to bound the distance between experiments by that of the relevant oracles.

In EasyCrypt, the adversary's internals aren't provided beyond its "main" procedure.
Instead, proofs are often formalized using the weaker notion of *coupling*:
roughly speaking, conditioned on the adversary's view being identical between two experiments, its next message is also the same.
Alternatively, for analyzing total variation distances directly,
the [EasyCrypt statistical distance library](https://github.com/EasyCrypt/easycrypt/blob/main/theories/distributions/SDist.ec) ("`SDist`") recovers a non-interactive adversary's next-message function using its output distribution.

The current treatment have proven to be sufficient for many applicationss.
In the CKKS analysis setting however,
it is unclear how to model data processing inequality for divergence in the module setting.
Moreover, there is no known way to extract an interactive adversary's next-message function where needed.

### Contributions

We generalize the `SDist` library to deal with parametrized distributions and adaptive adversaries.
Instead of relying on a next-message function,
we develop the idea of *partial coupling*.
At the high level, the idea can be viewed as a refinement of "up-to bad reasoning" for situations where it is difficult to write down a "bad event".
A similar idea of decomposing a distribution then constructing coupling for each component first appeared in ["Fixing and Mechanizing the Security Proof of Fiat-Shamir with Aborts and Dilithium"](https://ia.cr/2023/246).
The authors however did not consider the case where a coupling may not exist for a subset of the components.

We also tweak the MW18 proof to avoid other features that EasyCrypt does not provide.
For example, as one cannot easily rerun arbitrary adversaries,
we instantiate the experiments in the MW18 proof as neededd.
We also use the data processing inequality for total variation distance instead of divergences.

## Preliminary and Notations

Let $\Delta$ denote the total variation distance.
Let $\mathsf{Bern}(p)$ denote the Bernoulli distribution with parameter $0\le p\le1$.

For some fixed parameter $\lambda>0$, we call a function $\delta$ on two distributions a $\lambda$*-efficient measure* if it satisfies the following properties:
1. *Pythagorean probability preservation:*
   For any joint distributions $\mathcal{D}_i$ and $\mathcal{D}'_i$ over support $\Pi_i S_i$,
   if
   $$TODO$$
   for all $i$ and $a_i\in \Pi_i S_i$, then
   $$\Delta(TODO).$$
1. *Sub-additivity for joint distributions:*
   TODO...
1. *Data processing inequality:*
   For any distributions $\mathcal{D}$ and $\mathcal{D}'$ and any randomized algorithm $f$,
   we have $\delta(f(\mathcal{D}), f(\mathcal{D}'))\leq \delta(\mathcal{D}, \mathcal{D}')$.

For parametrized distributions $\mathcal{D}(\theta)$ and $\mathcal{D}'(\theta)$,
we abuse the notation and write the divergence between them
$$\delta(\mathcal{D}(\theta), \mathcal{D}'(\theta))=\max_\theta(\delta(\mathcal{D}(\theta), \mathcal{D}'(\theta)))$$
for the maximum divergence taken over non-adaptive choices of $\theta$.

### Proof of Lemma 3.3 in the Existing Literature

Let
$\varepsilon=\Pr[\mathsf{accept}\leftarrow\mathscr{G}(\mathcal{D})]$
be the accept probability of $\mathscr{G}$, and define $\varepsilon'$ similarly for $\mathscr{G}'$.
For $n\in\mathbb{N}$, define $\mathscr{G}_n$ as the $n$-fold repetition of $\mathscr{G}$ that outputs $\mathsf{accept}$ if any of the $n$ repetition does, and similarly for $\mathscr{G}_n'$.
We can then write the success probability of $\mathscr{G}_n$ as
$\varepsilon_n=1-(1-\varepsilon)^n$
and again similarly for $\varepsilon_n'$.

By data processing inequality, we have the bound
$$\varepsilon_n'\leq \varepsilon_n + \Delta(\set{(\theta_i, \mathcal{D}(\theta_i):1\leq i\leq nq},
\set{(\theta'_i, \mathcal{D}'(\theta'_i): 1\leq i\leq nq})$$
where $\theta_i$ is the parameter chosen for the $i$-th sample.
Using Pythagorean preservation of $\delta$, we have
$$\Delta(\set{(\theta_i, \mathcal{D}(\theta_i):1\leq i\leq nq},
\set{(\theta'_i, \mathcal{D}'(\theta'_i): 1\leq i\leq nq})\leq
\left|\left|\set{\max\_{< i}
\delta((\theta_i, \mathcal{D}(\theta_i)), (\theta_i', \mathcal{D}'(\theta'_i)))}\_{i=1}^{nq}\right|\right|_2$$
where the maximum is taken over fixed previous parameters and samples chosen.
Moreover, for fixed $\theta\_{ < i}$ and $\mathcal{D}(\theta\_{ < i})$, we have
$$\delta((\theta_i, \mathcal{D}(\theta_i)), (\theta_i', \mathcal{D}'(\theta'_i)) \leq \delta(\mathcal{D}(\theta_i), \mathcal{D}'(\theta_i))$$
as the two experiments behave identically up to the choice of $\mathcal{D}(\theta_i)$.
Collecting everything so far, we have
$\varepsilon_n'\leq \varepsilon_n + \sqrt{n \varepsilon}$.
We next apply union bound to obtain
$\varepsilon_n'\leq n\varepsilon + \sqrt{n \varepsilon}$.
Setting $n=\frac{1}{\varepsilon'}$, we also have
$\varepsilon_n'=1-(1-\frac{1}{n})^n>1-\frac{1}{e}$.
We finally conclude that
$1-\frac{1}{e}\leq\frac{\varepsilon'}{\varepsilon}+\sqrt{\frac{\varepsilon'}{\varepsilon}}$,
and therefore $\frac{\varepsilon'}{\varepsilon}>\frac{1}{8}$.

## Our Proofs

We propose some changes to the existing analysis to simplify formal verification.

### Treating Statistical Distance in the Interactive Setting

Before introducing our proof of Lemma 3.3, we first present a generalization of the data processing inequality to the interactive setting.
TODO say something about partial couplings
TODO formalize and write down the lemma statement.

Given two distributions $\mathcal{D}$ and $\mathcal{D}'$, we can construct a sub-distribution $\mathcal{F}\_{\vee}$ of their overlap where
$$\Pr[x\leftarrow\mathcal{F}\_\vee]=\min(\Pr[x\leftarrow\mathcal{D}\_1], \Pr[x\leftarrow\mathcal{D}\_2]).$$
We can then renormalize $\mathcal{F}\_\vee$ by first computing its weight
$$s=\sum TODO$$
and define $\mathcal{D}\_{\vee}$ by
$$\Pr[x\leftarrow\mathcal{D}\_\vee]=\frac{1}{s}\cdot \Pr[x\leftarrow\mathcal{F}\_\vee].$$
We can then rewrite
$$\mathcal{D}_1=TODO$$
by the chain rule and similarly for $\mathcal{D}_2$.
Operationally this shows we can sample from $\mathcal{D}_1$ by first sampling from the marginal $TODO$ and then sampling from the rest.
This naturally establishes a partial coupling between $\mathcal{D}_1$ and $\mathcal{D}_2$.

### Our Proof of Lemma 3.3

To run $n$ copies of the given experiment,
we begin by instantiating it simply as flipping $n=\frac{1}{\varepsilon'}$ biased coins of weights $\varepsilon$ and $\varepsilon'$ each.
We can then show that
$1-\frac{1}{e}\leq n\varepsilon + n \Delta(\mathsf{Bern}(\varepsilon), \mathsf{Bern}(\varepsilon'))$
by following similar argument as before.
As $\mathsf{Bern}(\varepsilon)$ is equivalent to $\mathscr{G}$ by definition, we have
$\Delta(\mathsf{Bern}(\varepsilon), \mathsf{Bern}(\varepsilon'))=\Delta(\mathscr{G}, \mathscr{G}')$.
By data processing inequality then, we can bound this total variation distance by the parameters chosen and samples drawn throughout the game.
$$\Delta(\mathscr{G}, \mathscr{G}')\leq
\Delta(\set{(\theta_i, \mathcal{D}(\theta_i):1\leq i\leq nq},
\set{(\theta'_i, \mathcal{D}'(\theta'_i): 1\leq i\leq nq})$$
We next apply our generalization of data processing inequality and obtain
$$\Delta(\set{(\theta_i, \mathcal{D}(\theta_i):1\leq i\leq q},
\set{(\theta'_i, \mathcal{D}'(\theta'_i): 1\leq i\leq q})\leq
\Delta(\set{(\mathcal{D}:1\leq i\leq q},
\set{(\mathcal{D}': 1\leq i\leq q}).$$
By some magic (TODO probably by instantiating the optimal distinguisher? Or maybe this is false and I'll be very, very sad.) we have
$$n\Delta(\set{(\mathcal{D}:1\leq i\leq q},
\set{(\mathcal{D}': 1\leq i\leq q})=
\Delta(\set{(\mathcal{D}:1\leq i\leq nq},
\set{(\mathcal{D}': 1\leq i\leq nq}).$$
We finally apply Pythagorean probability preservation to get
$$\Delta(\set{(\mathcal{D}:1\leq i\leq nq},
\set{(\mathcal{D}': 1\leq i\leq nq})\leq
\delta(\mathcal{D}, \mathcal{D}')$$
and the conclusion follows by collecting everything.

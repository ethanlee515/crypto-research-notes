# Relating KL Divergence to Cryptographic Game Hops

Here we outline our plans and efforts to formally verify a particularly tricky step in the CKKS security analysis.

## Overview

It is perhaps a common idea to replace a distribution with another similar one,
though there are variations in how this intuition is treated.
The theoretical cryptography community often measures the difference between two distributions using the total variation distance.
In our analysis of CKKS, however, the distance between distributions are taken as the KL-divergence.
We follow the analysis in ["Gaussian Sampling over the Integers: Efficient, Generic, Constant-Time"](https://ia.cr/2017/259) which uses a generalization of KL-divergence called $\lambda$*-efficient measures*.
We now state the main lemma we aim to prove.

**Lemma 3.3, rephrased**:
Let $\kappa\in\mathbb{Z}$ be the security parameter, and let $\delta$ be a $2^{-\kappa/2}$-efficient measure.
Let $\mathcal{D}(\theta)$ and $\mathcal{D}'(\theta)$ be parametrized distributions satisfying $\delta(\mathcal{D}(\theta), \mathcal{D}'(\theta))<2^{-\kappa/2}$.
Let $\mathscr{G}$ be an experiment drawing at most $q$ samples from $\mathcal{D}$ and then outputting either $\mathsf{accept}$ or $\mathsf{reject}$.
We moreover assume $\mathscr{G}$ satisfies that $\Pr[\mathsf{accept}\leftarrow\mathscr{G}]<2^{-\kappa}$.
Define $\mathscr{G}'$ to run $\mathscr{G}$ but sample from $\mathcal{D}'$ instead.
We can then bound the accepting probability of $\mathscr{G}'$ as $\Pr[\mathsf{accept}\leftarrow\mathscr{G}']<8\times 2^{-\kappa}$.

### Challenges

As we will see later, it is difficult to faithfully reproduce the lemma's proof using EasyCrypt.
For now we highlight some limitations of EasyCrypt.

#### Making Copies of an Adversary

Given an adversary in EasyCrypt, there is no way to clone and run many independent copies of it out of the box.
Unfortunately, the proof of Lemma 3.3 is one of the many proofs that requires precisely this.
The work ["Reflection, Rewinding, and Coin-Toss in EasyCrypt"](https://ia.cr/2021/1078) addresses this challenge.
It however requires adding additional *rewindable* assumptions to the adversary, and directly accessing the adversary's internal states using the `glob` feature which has historically been difficult to get right.

#### Treatment of Statistical Distance and Indistinguishability

In the analysis of cryptographic games, one often assumes without loss of generality that an adversary runs some "next message function" for each round of the interaction.
One then (sometimes implicitly) applies the data processing inequality to this function to bound the distance between experiments by that of the relevant oracles.

In EasyCrypt, the adversary's internals aren't provided beyond its "main" procedure.
Instead, proofs are often formalized using the weaker notion of *coupling*:
roughly speaking, conditioned on the adversary's view being identical between two experiments, its next message is also the same.
Alternatively, for analyzing total variation distances directly,
the [EasyCrypt statistical distance library](https://github.com/EasyCrypt/easycrypt/blob/main/theories/distributions/SDist.ec) ("`SDist`") recovers a non-interactive adversary's next-message function using its output distribution.

The current treatment have proven to be sufficient for many applicationss.
In the CKKS analysis setting however,
it is unclear how to model data processing inequality for divergence in the module setting.
Moreover, there is no known way to extract an interactive adversary's next-message function where needed.

### Contributions

TODO our proof is broken, needs fix

## Preliminary and Notations

Let $\Delta$ denote the total variation distance.
Let $\mathsf{Bern}(p)$ denote the Bernoulli distribution with parameter $0\le p\le1$.

For some fixed parameter $\lambda>0$, we call a function $\delta$ on two distributions a $\lambda$*-efficient measure* if it satisfies the following properties:
1. *Pythagorean probability preservation:*
   For any joint distributions $\mathcal{D}_i$ and $\mathcal{D}'_i$ over support $\Pi_i S_i$,
   if
   $$TODO$$
   for all $i$ and $a_i\in \Pi_i S_i$, then
   $$\Delta(TODO).$$
1. *Sub-additivity for joint distributions:*
   TODO...
1. *Data processing inequality:*
   For any distributions $\mathcal{D}$ and $\mathcal{D}'$ and any randomized algorithm $f$,
   we have $\delta(f(\mathcal{D}), f(\mathcal{D}'))\leq \delta(\mathcal{D}, \mathcal{D}')$.

For parametrized distributions $\mathcal{D}(\theta)$ and $\mathcal{D}'(\theta)$,
we abuse the notation and write the divergence between them
$$\delta(\mathcal{D}(\theta), \mathcal{D}'(\theta))=\max_\theta(\delta(\mathcal{D}(\theta), \mathcal{D}'(\theta)))$$
for the maximum divergence taken over non-adaptive choices of $\theta$.

### Proof of Lemma 3.3 in the Existing Literature

Let
$\varepsilon=\Pr[\mathsf{accept}\leftarrow\mathscr{G}(\mathcal{D})]$
be the accept probability of $\mathscr{G}$, and define $\varepsilon'$ similarly for $\mathscr{G}'$.
For $n\in\mathbb{N}$, define $\mathscr{G}_n$ as the $n$-fold repetition of $\mathscr{G}$ that outputs $\mathsf{accept}$ if any of the $n$ repetition does, and similarly for $\mathscr{G}_n'$.
We can then write the success probability of $\mathscr{G}_n$ as
$\varepsilon_n=1-(1-\varepsilon)^n$
and again similarly for $\varepsilon_n'$.

By data processing inequality, we have the bound
$$\varepsilon_n'\leq \varepsilon_n + \Delta(\set{(\theta_i, \mathcal{D}(\theta_i):1\leq i\leq nq},
\set{(\theta'_i, \mathcal{D}'(\theta'_i): 1\leq i\leq nq})$$
where $\theta_i$ is the parameter chosen for the $i$-th sample.
Using Pythagorean preservation of $\delta$, we have
$$\Delta(\set{(\theta_i, \mathcal{D}(\theta_i):1\leq i\leq nq},
\set{(\theta'_i, \mathcal{D}'(\theta'_i): 1\leq i\leq nq})\leq
\left|\left|\set{\max\_{< i}
\delta((\theta_i, \mathcal{D}(\theta_i)), (\theta_i', \mathcal{D}'(\theta'_i)))}\_{i=1}^{nq}\right|\right|_2$$
where the maximum is taken over fixed previous parameters and samples chosen.
Moreover, for fixed $\theta\_{ < i}$ and $\mathcal{D}(\theta\_{ < i})$, we have
$$\delta((\theta_i, \mathcal{D}(\theta_i)), (\theta_i', \mathcal{D}'(\theta'_i)) \leq \delta(\mathcal{D}(\theta_i), \mathcal{D}'(\theta_i))$$
as the two experiments behave identically up to the choice of $\mathcal{D}(\theta_i)$.
Collecting everything so far, we have
$\varepsilon_n'\leq \varepsilon_n + \sqrt{n \varepsilon}$.
We next apply union bound to obtain
$\varepsilon_n'\leq n\varepsilon + \sqrt{n \varepsilon}$.
Setting $n=\frac{1}{\varepsilon'}$, we also have
$\varepsilon_n'=1-(1-\frac{1}{n})^n>1-\frac{1}{e}$.
We finally conclude that
$1-\frac{1}{e}\leq\frac{\varepsilon'}{\varepsilon}+\sqrt{\frac{\varepsilon'}{\varepsilon}}$,
and therefore $\frac{\varepsilon'}{\varepsilon}>\frac{1}{8}$.

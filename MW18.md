# Relating KL-divergence to Cryptographic Game Hops

It is perhaps a common idea to replace a distribution with another similar one,
though there are variations in how this intuition is treated.
The theoretical cryptography community often measures the difference between two distributions using the total variation distance.
In our analysis of CKKS however, the distance between distributions are taken as the KL-divergence.
We follow the analysis in ["Gaussian Sampling over the Integers: Efficient, Generic, Constant-Time"](ia.cr/2017/259) which uses a generalization of KL-divergence called $\lambda$*-efficient measures*.
We now state the main lemma we aim to prove.

**Lemma 3.3, rephrased**:
Let $\kappa\in\mathbb{Z}$ be the security parameter and let $\delta$ be a $2^{-\kappa/2}$-efficient measure.
Let $\mathcal{D}(\theta)$ and $\mathcal{D}'(\theta)$ be parametrized distributions satisfying $\delta(\mathcal{D}(\theta), \mathcal{D}'(\theta))<2^{-\kappa/2}$.
Let $\mathscr{G}$ be an experiment drawing at most $q$ samples from $\mathcal{D}$ and then outputting either $\mathsf{accept}$ or $\mathsf{reject}$, and satisfies $\Pr[\mathsf{accept}\leftarrow\mathscr{G}]<2^{-\kappa}$.
Define $\mathscr{G}'$ to run $\mathscr{G}$ but sample from $\mathcal{D}'$ instead.
We can then bound the accepting probability of $\mathscr{G}'$ as $\Pr[\mathsf{accept}\leftarrow\mathscr{G}']<8\times 2^{-\kappa}$.

## Preliminary and Notations

Let $\Delta$ denote the total variation distance.

For some fixed parameter $\lambda>0$, we call a function $\delta$ on two distributions a $\lambda$*-efficient measure* if it satisfies the following properties:
1. *Pythagorean probability preservation:*
   For any joint distributions $\mathcal{D}_i$ and $\mathcal{D}'_i$ over support $\Pi_i S_i$,
   if
   $$TODO$$
   for all $i$ and $a_i\in \Pi_i S_i$, then
   $$\Delta(TODO).$$
1. *Sub-additivity for joint distributions:*
   TODO...
1. *Data processing inequality:*
   For any distributions $\mathcal{D}$ and $\mathcal{D}'$ and any randomized algorithm $f$,
   we have $\delta(f(\mathcal{D}), f(\mathcal{D}'))\leq \delta(\mathcal{D}, \mathcal{D}')$.

For parametrized distributions $\mathcal{D}(\theta)$ and $\mathcal{D}'(\theta)$,
we abuse the notation and write the divergence between them
$$\delta(\mathcal{D}(\theta), \mathcal{D}'(\theta))=\max_\theta(\delta(\mathcal{D}(\theta), \mathcal{D}'(\theta)))$$
for the maximum divergence taken over non-adaptive choices of $\theta$.

## The Existing Proof

We now present the proof of Lemma 3.3 from the existing literature.

### The Proof of Lemma 3.3

Let
$\varepsilon=\Pr[\mathsf{accept}\leftarrow\mathscr{G}(\mathcal{X}_1)]$
be the accept probability of $\mathscr{G}$ and define $\varepsilon'$ similarly for $\mathscr{G}'$.
For $n\in\mathbb{N}$, define $\mathscr{G}_n$ as the $n$-fold repetition of $\mathscr{G}$ that outputs $\mathsf{accept}$ if any of the $n$ repetition does, and similarly for $\mathscr{G}_n'$.
We can then write the success probability of $\mathscr{G}_n$ as
$\varepsilon_n=1-(1-\varepsilon)^n$
and again similarly for $\varepsilon_n'$.

By data processing inequality, we have the bound
$$\varepsilon_n'\leq \varepsilon_n + \Delta(\set{(\theta_i, \mathcal{D}(\theta_i):1\leq i\leq nq},
\set{(\theta'_i, \mathcal{D}'(\theta'_i): 1\leq i\leq nq})$$
where $\theta_i$ is the parameter chosen for the $i$-th sample.
Using Pythagorean preservation of $\delta$, we have
$$\Delta(\set{(\theta_i, \mathcal{D}(\theta_i):1\leq i\leq nq},
\set{(\theta'_i, \mathcal{D}'(\theta'_i): 1\leq i\leq nq})\leq
\left|\left|\set{\max\_{< i}
\delta((\theta_i, \mathcal{D}(\theta_i)), (\theta_i', \mathcal{D}'(\theta'_i)))}\_{i=1}^{nq}\right|\right|_2$$
where the maximum is taken over fixed previous parameters and samples chosen.
Moreover, for fixed $\theta\_{ < i}$ and $\mathcal{D}(\theta\_{ < i})$, we have
$$\delta((\theta_i, \mathcal{D}(\theta_i)), (\theta_i', \mathcal{D}'(\theta'_i)) \leq \delta(\mathcal{D}(\theta_i), \mathcal{D}'(\theta_i))$$
as the two experiments behave identically up to the choice of $\mathcal{D}(\theta_i)$.
Collecting everything so far together, we have
$\varepsilon_n'\leq \varepsilon_n + \sqrt{n \varepsilon}$.
We next use union bound to get
$\varepsilon_n'\leq n\varepsilon + \sqrt{n \varepsilon}$.
Setting $n=\frac{1}{\varepsilon'}$, we also have
$\varepsilon_n'=1-(1-\frac{1}{n})^n>1-\frac{1}{e}$.
We now conclude that
$1-\frac{1}{e}\leq\frac{\varepsilon'}{\varepsilon}+\sqrt{\frac{\varepsilon'}{\varepsilon}}$,
and therefore $\frac{\varepsilon'}{\varepsilon}>\frac{1}{8}$.

## Overview on Formally Verifying the Proof

### Challenges

1. Parametrized distribution... A priori the adversary can be adaptive and maybe proving TVD preservation gets unfun?
1. Given an adversary in EasyCrypt, there are no easy way to clone and run n independent copies of it.
It is possible that the [rewinding paper](ia.cr/2021/1078) might be useful to reset the adversary between runs but we don't know. (Maybe it adds extra assumptions on the adversary and complicates the interface?)
1. Maybe no clean way to show data processing inequality for divergence in the module setting? Maybe RHL abstract procedure call invariant rules are TVD only?

### Our Strategy

Hopefully this works?

1. Generalize EasyCrypt SDist library to deal with inputs to the oracle?
1. instantiate $n$ coin flips so we can repeat it in parallel
1. Use data processing to go from TVD of coin flips to TVD of samples (TODO might have to instantiate the optimal distinguisher here?)
1. Use Pythagorean preservation to connect the dots back to the MW18 proof

## Treating parametrized distrs

### The EasyCrypt `SDist` library

EasyCrypt provides a [library](https://github.com/EasyCrypt/easycrypt/blob/main/theories/distributions/SDist.ec) for the total variation distance.
Roughly speaking, it defines total variation distance between distributions and identifies the adversary's actions with distributions over corresponding outputs.
Unfortunately, this strategy does not generalize easily to a situation when there are inputs to the oracle calls and where the adversary is (in principle, syntactically) adaptive.
It is complicated to model such an adversary. On pen and paper, this amounts to:
1. assume without the loss of generality that the adversary is deterministic
2. writing down the next-message functions of the adversary

The first step might be tedious to prove in full generality, and the second step gives a very unwieldy model of the adversary.

### Dealing with SDist in the adaptive setting?

Here we propose instead to use the chain rule of probability to construct partial couplings.
We observe that given two distributions $\mathcal{D}_1$ and $\mathcal{D}_2$, we can construct a sub-distribution $\mathcal{D}\_{\vee}$ of their overlap as $\mathcal{Q}$ where
$$\Pr[x\leftarrow\mathcal{F}\_\vee]=\min(\Pr[x\leftarrow\mathcal{D}\_1], \Pr[x\leftarrow\mathcal{D}\_2]).$$
We can then renormalize $\mathcal{F}\_\vee$ by first computing its weight
$$s=\sum TODO$$
and then define $\mathcal{D}\_{\vee}$ by
$$\Pr[x\leftarrow\mathcal{D}\_\vee]=\frac{1}{s}\cdot \Pr[x\leftarrow\mathcal{F}\_\vee].$$
We can then rewrite
$$\mathcal{D}_1=TODO$$
by the chain rule and similarly for $\mathcal{D}_2$.
Operationally this implies that we can sample from $\mathcal{D}_1$ by first sampling from the marginal $TODO$ and then sampling from the rest.
This naturally establishes a partial coupling between $\mathcal{D}_1$ and $\mathcal{D}_2$.

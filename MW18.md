# Relating KL Divergence to Cryptographic Game Hops

Here we outline our plans and efforts to formally verify a particularly tricky step in the CKKS proof.

## Overview

It is perhaps a common idea to replace a distribution with another similar one,
though there are variations in how this intuition is treated.
The theoretical cryptography community often measures the difference between two distributions using the total variation distance.
In our analysis of CKKS, however, the distance between distributions are taken as the KL-divergence.
We follow the analysis in ["Gaussian Sampling over the Integers: Efficient, Generic, Constant-Time"](ia.cr/2017/259) which uses a generalization of KL-divergence called $\lambda$*-efficient measures*.
We now state the main lemma we aim to prove.

**Lemma 3.3, rephrased**:
Let $\kappa\in\mathbb{Z}$ be the security parameter, and let $\delta$ be a $2^{-\kappa/2}$-efficient measure.
Let $\mathcal{D}(\theta)$ and $\mathcal{D}'(\theta)$ be parametrized distributions satisfying $\delta(\mathcal{D}(\theta), \mathcal{D}'(\theta))<2^{-\kappa/2}$.
Let $\mathscr{G}$ be an experiment drawing at most $q$ samples from $\mathcal{D}$ and then outputting either $\mathsf{accept}$ or $\mathsf{reject}$, and satisfies $\Pr[\mathsf{accept}\leftarrow\mathscr{G}]<2^{-\kappa}$.
Define $\mathscr{G}'$ to run $\mathscr{G}$ but sample from $\mathcal{D}'$ instead.
We can then bound the accepting probability of $\mathscr{G}'$ as $\Pr[\mathsf{accept}\leftarrow\mathscr{G}']<8\times 2^{-\kappa}$.

### Challenges

As we will see later, it is difficult to faithfully reproduce the lemma's proof using EasyCrypt.
For now we highlight some limitations of EasyCrypt.

#### Running Multiple Independent Copies of an Adversary

Given an adversary in EasyCrypt, there is no built-in way to clone and run many independent copies of it.
Unfortunately, the proof of Lemma 3.3 is one of the many proofs that require precisely this.
The work ["Reflection, Rewinding, and Coin-Toss in EasyCrypt"](ia.cr/2021/1078) provides a possible solution.
It however requires adding additional *rewindable* assumptions to the adversary, and directly accessing the adversary's internal states using the `glob` feature which has historically been difficult to get right.

#### Treatment of Statistical Distance and Indistinguishability

In a typical pen-and-paper proof, one often assumes without loss of generality that an adversary runs some "next message function" for every round of the communication.
One then (sometimes implicitly) applies the data processing inequality to this function to bound the distance between experiments by that of the relevant oracles.

In EasyCrypt, the adversary's internals aren't provided beyond its "main" procedure.
Instead, proofs are often formalized using the weaker notion of *coupling*:
roughly speaking, conditioned on the adversary's view being identical between two experiments, its next message is also the same.
Alternatively, when one needs to analyze total variation distances directly,
the [EasyCrypt statistical distance library](https://github.com/EasyCrypt/easycrypt/blob/main/theories/distributions/SDist.ec) ("`SDist`") recovers a non-interactive adversary's next-message function using its output distribution.

The current strategies have more than demonstrated their purpose.
In the CKKS analysis setting however,
it is unclear how to model data processing inequality for divergence in the module setting.
Moreover, it is (a priori) unclear how to generalize the `SDist` library to analyze the total variation distance between two interactive experiments.

### Contributions

We generalize the `SDist` library to deal with parametrized distributions and adaptive adversaries.
Instead of defining the next-message function,
we explore the concept of *partial coupling* using the chain rule of probability.

We also tweak the MW18 proof to avoid features that EasyCrypt does not provide.
For example, as one cannot easily rerun abstract adversaries,
we instantiate the adversary in the MW18 proof appropriately.
We also use the data processing inequality for total variation distance instead of divergences.

## Preliminary and Notations

Let $\Delta$ denote the total variation distance.

For some fixed parameter $\lambda>0$, we call a function $\delta$ on two distributions a $\lambda$*-efficient measure* if it satisfies the following properties:
1. *Pythagorean probability preservation:*
   For any joint distributions $\mathcal{D}_i$ and $\mathcal{D}'_i$ over support $\Pi_i S_i$,
   if
   $$TODO$$
   for all $i$ and $a_i\in \Pi_i S_i$, then
   $$\Delta(TODO).$$
1. *Sub-additivity for joint distributions:*
   TODO...
1. *Data processing inequality:*
   For any distributions $\mathcal{D}$ and $\mathcal{D}'$ and any randomized algorithm $f$,
   we have $\delta(f(\mathcal{D}), f(\mathcal{D}'))\leq \delta(\mathcal{D}, \mathcal{D}')$.

For parametrized distributions $\mathcal{D}(\theta)$ and $\mathcal{D}'(\theta)$,
we abuse the notation and write the divergence between them
$$\delta(\mathcal{D}(\theta), \mathcal{D}'(\theta))=\max_\theta(\delta(\mathcal{D}(\theta), \mathcal{D}'(\theta)))$$
for the maximum divergence taken over non-adaptive choices of $\theta$.

## The Existing Proof

We now present the proof of Lemma 3.3 from the existing literature.

### The Proof of Lemma 3.3

Let
$\varepsilon=\Pr[\mathsf{accept}\leftarrow\mathscr{G}(\mathcal{X}_1)]$
be the accept probability of $\mathscr{G}$ and define $\varepsilon'$ similarly for $\mathscr{G}'$.
For $n\in\mathbb{N}$, define $\mathscr{G}_n$ as the $n$-fold repetition of $\mathscr{G}$ that outputs $\mathsf{accept}$ if any of the $n$ repetition does, and similarly for $\mathscr{G}_n'$.
We can then write the success probability of $\mathscr{G}_n$ as
$\varepsilon_n=1-(1-\varepsilon)^n$
and again similarly for $\varepsilon_n'$.

By data processing inequality, we have the bound
$$\varepsilon_n'\leq \varepsilon_n + \Delta(\set{(\theta_i, \mathcal{D}(\theta_i):1\leq i\leq nq},
\set{(\theta'_i, \mathcal{D}'(\theta'_i): 1\leq i\leq nq})$$
where $\theta_i$ is the parameter chosen for the $i$-th sample.
Using Pythagorean preservation of $\delta$, we have
$$\Delta(\set{(\theta_i, \mathcal{D}(\theta_i):1\leq i\leq nq},
\set{(\theta'_i, \mathcal{D}'(\theta'_i): 1\leq i\leq nq})\leq
\left|\left|\set{\max\_{< i}
\delta((\theta_i, \mathcal{D}(\theta_i)), (\theta_i', \mathcal{D}'(\theta'_i)))}\_{i=1}^{nq}\right|\right|_2$$
where the maximum is taken over fixed previous parameters and samples chosen.
Moreover, for fixed $\theta\_{ < i}$ and $\mathcal{D}(\theta\_{ < i})$, we have
$$\delta((\theta_i, \mathcal{D}(\theta_i)), (\theta_i', \mathcal{D}'(\theta'_i)) \leq \delta(\mathcal{D}(\theta_i), \mathcal{D}'(\theta_i))$$
as the two experiments behave identically up to the choice of $\mathcal{D}(\theta_i)$.
Collecting everything so far, we have
$\varepsilon_n'\leq \varepsilon_n + \sqrt{n \varepsilon}$.
We next use union bound to get
$\varepsilon_n'\leq n\varepsilon + \sqrt{n \varepsilon}$.
Setting $n=\frac{1}{\varepsilon'}$, we also have
$\varepsilon_n'=1-(1-\frac{1}{n})^n>1-\frac{1}{e}$.
We now conclude that
$1-\frac{1}{e}\leq\frac{\varepsilon'}{\varepsilon}+\sqrt{\frac{\varepsilon'}{\varepsilon}}$,
and therefore $\frac{\varepsilon'}{\varepsilon}>\frac{1}{8}$.

## Overview on Formally Verifying the Proof

### Our Strategy

Hopefully this works?



### Treating parametrized distrs

#### The EasyCrypt `SDist` library

#### Dealing with SDist in the adaptive setting?

Here we propose instead to use the chain rule of probability to construct partial couplings.
We observe that given two distributions $\mathcal{D}_1$ and $\mathcal{D}_2$, we can construct a sub-distribution $\mathcal{D}\_{\vee}$ of their overlap as $\mathcal{Q}$ where
$$\Pr[x\leftarrow\mathcal{F}\_\vee]=\min(\Pr[x\leftarrow\mathcal{D}\_1], \Pr[x\leftarrow\mathcal{D}\_2]).$$
We can then renormalize $\mathcal{F}\_\vee$ by first computing its weight
$$s=\sum TODO$$
and then define $\mathcal{D}\_{\vee}$ by
$$\Pr[x\leftarrow\mathcal{D}\_\vee]=\frac{1}{s}\cdot \Pr[x\leftarrow\mathcal{F}\_\vee].$$
We can then rewrite
$$\mathcal{D}_1=TODO$$
by the chain rule and similarly for $\mathcal{D}_2$.
Operationally this implies that we can sample from $\mathcal{D}_1$ by first sampling from the marginal $TODO$ and then sampling from the rest.
This naturally establishes a partial coupling between $\mathcal{D}_1$ and $\mathcal{D}_2$.

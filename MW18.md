# Relating KL-divergence to Cryptographic Game Hops

It is perhaps a common idea to substitute a distrubution with a similar one,
though there are variations in how this intuition is formally modelled between communities.
The theoretical cryptography community often measures the difference between two distributions using the total variation distance.
In our analysis of full homomorphic encryption however, the distance between distributions are taken as the KL-divergence,
and the distributions are parametric.
We follow the analysis in ["Gaussian Sampling over the Integers: Efficient, Generic, Constant-Time"](ia.cr/2017/259) which uses a generalization of KL-divergence called $\lambda$*-efficient measures*.

**Lemma 3.3 of MW18, rephrased**:
Let $\kappa\in\mathbb{Z}$ be the security parameter and let $\delta$ be a $2^{-\kappa/2}$-efficient measure.
Let $\mathcal{X}(\theta)$ and $\mathcal{X}'(\theta)$ be parametrized distributions satisfying $\delta(\mathcal{X}(\theta), \mathcal{X}'(\theta))<2^{-\kappa/2}$.
Let $\mathscr{G}$ be an experiment drawing at most $q$ samples from $\mathcal{X}$ and then outputting either $\mathsf{accept}$ or $\mathsf{reject}$, and $\Pr[\mathsf{accept}\leftarrow\mathscr{G}]<2^{-\kappa}$.
Define $\mathscr{G}'$ to run $\mathscr{G}$ but sample from $\mathcal{X}'$ instead.
We then have the upper bound $\Pr[\mathsf{accept}\leftarrow\mathscr{G}']<8\times 2^{-\kappa}$.

## Preliminary and Notations

Let $\delta$ be a divergence and $\Delta$ be the total variation distance.
We then assume $\delta satisfy the following properties
1. Pythagorean...?
2. TODO I forgor
3. I forgor

For parametrized distributions $\mathcal{D}(\theta)$ and $\mathcal{D}'(\theta)$,
we define the divergence between them
$$\delta(\mathcal{D}(\theta), \mathcal{D}'(\theta))=\max(TODO)$$
as the maximum divergence over the ensemble.

## Provided proof

Here we sketch the proof provided in MW18, with relatively minor modifications.

Let
$$\epsilon=\Pr[\mathsf{accept}\leftarrow\mathscr{G}(\mathcal{X}_1)]$$
be the accept probability of $\mathscr{G}$ and define $\epsilon'$ similarly for $\mathscr{G}'$.
For $n\in\mathbb{N}$, define $\mathscr{G}_n$ as the $n$-fold repetition of $\mathscr{G}$ that outputs $\mathsf{accept}$ if any of the $n$ repetition does.


## Challenges

Given an adversary, there are no easy way to clone and run n independent copies of it.
It is possible that the [rewinding paper](ia.cr/2021/1078) might be useful to reset the adversary between runs but we don't know.

## Our changes to the proof

1. Instantiate the adversary as the optimal one and run n independent copies of that
2. Prove that my adversary is indeed optimal

We start by giving a description of the optimal adversary...
TODO

## Analyzing Total Variation Distance in EasyCrypt

TODO exposition?

### The EasyCrypt `SDist` library

EasyCrypt provides a [library](https://github.com/EasyCrypt/easycrypt/blob/main/theories/distributions/SDist.ec) for the total variation distance.
Roughly speaking, it defines total variation distance between distributions and identifies the adversary's actions with distributions over corresponding outputs.
Unfortunately, this strategy does not generalize easily to a situation when there are inputs to the oracle calls and where the adversary is (in principle, syntactically) adaptive.
It is complicated to model such an adversary. On pen and paper, this amounts to:
1. assume without the loss of generality that the adversary is deterministic
2. writing down the next-message functions of the adversary

The first step might be tedious to prove in full generality, and the second step gives a very unwieldy model of the adversary.

### Our strategy

Here we propose instead to use the chain rule of probability to construct partial couplings.
We observe that given two distributions $\mathcal{D}_1$ and $\mathcal{D}_2$, we can construct a sub-distribution $\mathcal{D}\_{\vee}$ of their overlap as $\mathcal{Q}$ where
$$\Pr[x\leftarrow\mathcal{F}\_\vee]=\min(\Pr[x\leftarrow\mathcal{D}\_1], \Pr[x\leftarrow\mathcal{D}\_2]).$$
We can then renormalize $\mathcal{F}\_\vee$ by first computing its weight
$$s=\sum TODO$$
and then define $\mathcal{D}\_{\vee}$ by
$$\Pr[x\leftarrow\mathcal{D}\_\vee]=\frac{1}{s}\cdot \Pr[x\leftarrow\mathcal{F}\_\vee].$$
We can then rewrite
$$\mathcal{D}_1=TODO$$
by the chain rule and similarly for $\mathcal{D}_2$.
Operationally this implies that we can sample from $\mathcal{D}_1$ by first sampling from the marginal $TODO$ and then sampling from the rest.
This naturally establishes a partial coupling between $\mathcal{D}_1$ and $\mathcal{D}_2$.

From this coupling, we can then establish the invariant

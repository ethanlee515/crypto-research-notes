\documentclass{article}

\include{macros}

\title{Noise Flooding with Discrete Gaussians}
\author{}

\begin{document}

\maketitle

\section{Overview}

We want to bound the distance between two discrete Gaussians in terms of their means and variances.
We first give a top-down overview of the analysis before diving into details.
Specifically, we state all the lemma used in the analysis and defer their proofs for later.
We begin by defining the building blocks for our main theorem.

\begin{definition}[Discrete Gaussian]
	For any $\mu\in\bbZ$ and $\sigma\in\bbR$, denote $\DG{\mu}{\sigma^2}$ as the discrete Gaussian distribution centered at $\mu$ with variance $\sigma^2$.
	To write down the pdf, for all $x\in\bbZ$ we have
	\begin{equation}
		\Pr\sqparens{\DG{\mu}{\sigma^2}=x}=\frac{1}{C}e^{-\parens{x-\mu}^2/2\sigma^2}
	\end{equation}
	for normalization constant
	\begin{equation}
		C=\sum_{x\in\bbZ} e^{-\parens{x-\mu}^2/2\sigma^2}
	\end{equation}
\end{definition}

\begin{definition}[Kullbackâ€“Leibler divergence]
	Let $\cP$ and $\cQ$ be distributions over integers.
	We define the KL divergence to be
	\begin{equation}
		\KL{\cP}{\cQ} = \sum_{x\in\bbZ}\Pr\sqparens{\cP = x}\cdot
		\ln\parens{\frac{\Pr\sqparens{\cP=x}}{\Pr\sqparens{\cQ=x}}}
	\end{equation}
\end{definition}

We now state precisely the theorem we want to prove.

\begin{theorem}
	For all $\mu, \nu\in\bbZ$ and $\sigma\in\bbR$, we have
	\begin{equation}
		\KL{\DG{\mu}{\sigma^2}}{\DG{\nu}{\sigma^2}} = \frac{\parens{\nu - \mu}^2}{2\sigma^2}
	\end{equation}
\end{theorem}

To show this, we use a relation between KL divergence and R\'enyi divergence.
We first define the R\'enyi divergence.
\begin{definition}[R\'enyi divergence]
	Let $\cP$ and $\cQ$ be distributions over integers, and let $\alpha\in\bbR$.
	We define
	\begin{equation}
		\renyi{\cP}{\cQ} = \frac{1}{\alpha - 1}
		\ln\parens{\sum_{x\in\bbZ}
		\parens{\Pr\sqparens{\cP=x}}^\alpha
		\parens{\Pr\sqparens{\cQ=x}}^{1-\alpha}}
	\end{equation}
\end{definition}

We can then write KL divergence in terms of R\'enyi divergence:
\begin{lemma}
	For all $\cP$ and $\cQ$ distributions over integers,
	\begin{equation}
		\KL{\cP}{\cQ} = \lim_{\alpha\rightarrow 1}\renyi{\cP}{\cQ}
	\end{equation}
\end{lemma}

Thus it suffices to show that
\begin{lemma}
	\label{lemma:DG-renyi-bound}
	Let $\cP$ and $\cQ$ be distributions over integers, and let $\alpha, \sigma\in\bbR$. We have
	\begin{equation}
		\renyi{\DG{\mu}{\sigma^2}}{\DG{\nu}{\sigma^2}} \leq \frac{\parens{\nu - \mu}^2}{2\sigma^2} \cdot \alpha
	\end{equation}
\end{lemma}

The proof of \Cref{lemma:DG-renyi-bound} uses the following bound for the $\ell_1$-norm of the shifted discrete Gaussian:
\begin{lemma}
	\label{lemma:norm-shifted-DG}
	For all $\mu\in\bbZ$ and $\sigma\in\bbR$ we have
	\begin{equation}
		\sum_{x\in\bbZ}e^{-\parens{x-\mu}^2 / 2\sigma^2} \leq \sum_{x\in\bbZ}e^{x^2 / 2\sigma^2} 
	\end{equation}
\end{lemma}

The proof of \Cref{lemma:norm-shifted-DG} follows from the Poisson summation formula.
We first define Fourier coefficients.
\begin{definition}
	Let $f:\bbR\rightarrow\bbC$ be a Schwarz function, then we write
	\begin{equation}
		\hat{f}(k)=\int_{-\infty}^{\infty} f(x)e^{-2\pi i k x} dx
	\end{equation}
\end{definition}

Now we state the Poisson summation formula.
\begin{lemma}[Poisson summation]
	\label{lemma:Poisson-summation}
	Let $f:\bbR\rightarrow\bbC$ be a Schwarz function, then we have
	\begin{equation}
		\sum_{n\in\bbZ}f(n)=\sum_{k\in\bbZ}\hat{f}(k)
	\end{equation}
\end{lemma}

As one may expect, \Cref{lemma:Poisson-summation} can be shown using standard results in real analysis and Fourier analysis.
First we state a sufficient condition for exchanging sums and integrals.
\begin{theorem}
	\label{theorem:exchange-sum-and-integral}
	Let $F_k:\bbR\rightarrow\bbC$ such that $F_k\rightarrow F$ uniformly for some $F$, then we have $\int F_k\rightarrow \int F$.
\end{theorem}
We also need convergence of Fourier series.
There are many different criteria and types of convergence for Fourier series;
here we use the following version.
\begin{theorem}
	\label{theorem:Fourier-converge}
	Let $f$ be continuous and periodic on $[0, 1)$, and
	\begin{equation}
		\sum_{k\in\bbZ}\abs{\hat{f}(k)}<\infty,
	\end{equation}
	then for all $x\in\bbR$,
	\begin{equation}
		f(x)=\sum_{k\in\bbZ}\hat{f}(k)e^{2\pi ikx}
	\end{equation}
\end{theorem}
Finally, the convergence theorem above uses the fact that Fourier series is unique.
\begin{theorem}
	\label{theorem:Fourier-unique}
	Suppose $f$ is continuous and periodic on $[0, 1)$, and satisfies $\int f<\infty$ and also $\hat{f}(k)=0$ for all $k\in\bbZ$, then $f=0$.
\end{theorem}

\section{Proofs of the lemmas}

We present the proofs in reverse order, except we omit the proof for \Cref{theorem:exchange-sum-and-integral} since it's obviously true for Riemann integrals.
First, \Cref{theorem:Fourier-unique} and \Cref{theorem:Fourier-converge} are standard material found in textbooks such as \cite{fourier-textbook}.
We include their proofs here for completeness.

\begin{proof}[Proof of \Cref{theorem:Fourier-unique}]
	We assume without loss of generality that $f$ is defined on $[-\pi, \pi]$.
	Suppose for the sake of contradiction that $f(0)>0$; this choice of nonzero point is also without loss of generality since Fourier series have properties for horizontal shifts.
	Since $f$ is continuous then, there exists some neighborhood $\delta<\frac{\pi}{2}$ so that $f(x)>\frac{y}{2}$ for all $x\in(x_0 - \delta, x_0 + \delta)$.
	We next define
	\begin{equation}
		p(\theta)=\varepsilon+\cos\theta
	\end{equation}
	where
	$$\varepsilon=\frac{1}{4}\parens{1-\cos\delta}$$
	is chosen so for all $\delta\leq\abs{\theta}\leq\pi$ we have $\abs{p\parens{\theta}}<1-\frac{\varepsilon}{2}$.
	We next choose $0<\eta<\delta$ so that $p(\eta)\geq 1 +\frac{\varepsilon}{2}$ which we can do due to continuity of $p$.
	Notice that
	$$p_k\parens{\theta}=\sqparens{p\parens{\theta}}^k$$
	is a trigonometric polynomial, so we have
	\begin{equation*}
		\int_{-\pi}^\pi f(\theta) p_k(\theta)d\theta = 0.
	\end{equation*}
	We however see that can't be true, since as $k\rightarrow\infty$ we have
	\begin{equation*}
		\int_{\abs{\theta}<\eta}f(\theta)p_k(\theta)\geq 2\eta\frac{f(0)}{2}\parens{1+\frac{\varepsilon}{2}}^k\rightarrow\infty
	\end{equation*}
	while
	\begin{equation*}
		\int_{\eta\leq\abs{\theta}<\delta}f(\theta)p_k(\theta)\geq 0
	\end{equation*}
	and the rest of the integral is finite for continuous and integrable (and therefore bounded) $f$.
\end{proof}

\begin{proof}[Proof of \Cref{theorem:Fourier-converge}]
TODO
\end{proof}

\subsection{A Tail Bound for Discrete Gaussian}

Using the machinery developed so far,
we can also easily show a tail-bound for the discrete Gaussian.
One simply shows that discrete Gaussian is a \emph{subgaussian} distribution,
then use the tail bounds for subgaussian distributions.

\begin{definition}[subgaussian distributions]
	TODO
\end{definition}

We show that the discrete Gaussian satisfies the Laplace tail \Ethan{?} property...
\begin{lemma}
	TODO
\end{lemma}

Next we give a standard result that Laplace property implies the tail...
\begin{lemma}
	TODO
\end{lemma}

\bibliographystyle{plain}
\bibliography{references}

\end{document}

\documentclass{article}

\include{macros}

\title{Noise Flooding with Discrete Gaussians}
\author{}

\begin{document}

\maketitle

\section{Overview}

We want to bound the distance between two discrete Gaussians in terms of their means and variances.
We first sketch the analysis by stating all the lemma involved, and defer their proofs for later.
We begin by defining the building blocks for our main theorem.

\begin{definition}[Discrete Gaussian]
	For any $\mu\in\bbZ$ and $\sigma\in\bbR$, denote $\DG{\mu}{\sigma^2}$ as the discrete Gaussian distribution centered at $\mu$ with variance $\sigma^2$.
	More concretely, for all $x\in\bbZ$ we can compute the norm of the Gaussian function
	\begin{equation}
		C=\sum_{x\in\bbZ} e^{-\parens{x-\mu}^2/2\sigma^2}
	\end{equation}
	and define the density function accordingly as
	\begin{equation}
		\Pr\sqparens{\DG{\mu}{\sigma^2}=x}=\frac{1}{C}e^{-\parens{x-\mu}^2/2\sigma^2}
	\end{equation}
\end{definition}

\begin{definition}[Kullbackâ€“Leibler divergence]
	Let $\cP$ and $\cQ$ be distributions over integers.
	We define the KL divergence to be
	\begin{equation}
		\KL{\cP}{\cQ} = \sum_{x\in\bbZ}\Pr\sqparens{\cP = x}\cdot
		\ln\parens{\frac{\Pr\sqparens{\cP=x}}{\Pr\sqparens{\cQ=x}}}
	\end{equation}
\end{definition}

We now state precisely the theorem we want to prove.

\begin{theorem}
	\label{theorem:noise-flooding}
	For all $\mu, \nu\in\bbZ$ and $\sigma\in\bbR$, we have
	\begin{equation}
		\KL{\DG{\mu}{\sigma^2}}{\DG{\nu}{\sigma^2}} = \frac{\parens{\nu - \mu}^2}{2\sigma^2}
	\end{equation}
\end{theorem}

To show this, we use a relation between KL divergence and R\'enyi divergence.
We first define the R\'enyi divergence.
\begin{definition}[R\'enyi divergence]
	Let $\cP$ and $\cQ$ be distributions over integers, and let $\alpha\ne 1$.
	We define
	\begin{equation}
		\renyi{\cP}{\cQ} = \frac{1}{\alpha - 1}
		\ln\parens{\sum_{x\in\bbZ}
		\parens{\Pr\sqparens{\cP=x}}^\alpha
		\parens{\Pr\sqparens{\cQ=x}}^{1-\alpha}}
	\end{equation}
\end{definition}

We can write KL divergence in terms of R\'enyi divergence.
Though this is generally true even for continuous distributions,
we assume the distributions are discrete Gaussians to simplify some arguments.
\begin{lemma}
	\label{lemma:KL-renyi}
	For discrete Gaussians distributions $\cP$ and $\cQ$,
	\begin{equation}
		\KL{\cP}{\cQ} = \lim_{\alpha\rightarrow 1}\renyi{\cP}{\cQ}
	\end{equation}
	if the limit exists.
\end{lemma}

Thus it suffices to show that
\begin{lemma}
	\label{lemma:DG-renyi-bound}
	Let $\cP$ and $\cQ$ be distributions over integers, let $\sigma\in\bbR$, and let $\alpha\ne 1$. We have
	\begin{equation}
		\renyi{\DG{\mu}{\sigma^2}}{\DG{\nu}{\sigma^2}} \leq \frac{\parens{\nu - \mu}^2}{2\sigma^2} \cdot \alpha
	\end{equation}
\end{lemma}

The proof of \Cref{lemma:DG-renyi-bound} uses the following bound for the $\ell_1$-norm of the shifted discrete Gaussian:
\begin{lemma}
	\label{lemma:norm-shifted-DG}
	For all $\mu\in\bbZ$ and $\sigma\in\bbR$ we have
	\begin{equation}
		\sum_{x\in\bbZ}e^{-\parens{x-\mu}^2 / 2\sigma^2} \leq \sum_{x\in\bbZ}e^{x^2 / 2\sigma^2} 
	\end{equation}
\end{lemma}

The proof of \Cref{lemma:norm-shifted-DG} follows from the Poisson summation formula.
We first define Fourier coefficients.
\begin{definition}
	Let $f:\bbR\rightarrow\bbC$ be a Schwarz function, then we write
	\begin{equation}
		\hat{f}(k)=\int_{-\infty}^{\infty} f(x)e^{-2\pi i k x} dx
	\end{equation}
\end{definition}

Now we state the Poisson summation formula.
\begin{lemma}[Poisson summation]
	\label{lemma:Poisson-summation}
	Let $f:\bbR\rightarrow\bbC$ be a Schwarz function, then we have
	\begin{equation}
		\sum_{n\in\bbZ}f(n)=\sum_{k\in\bbZ}\hat{f}(k)
	\end{equation}
\end{lemma}

As one may expect, \Cref{lemma:Poisson-summation} can be shown using standard results in real analysis and Fourier analysis.
First we state a sufficient condition for exchanging sums and integrals.
\begin{theorem}
	\label{theorem:exchange-sum-and-integral}
	Let $F_k:\bbR\rightarrow\bbC$ such that $F_k\rightarrow F$ uniformly for some $F$, then we have $\int F_k\rightarrow \int F$.
\end{theorem}
We also need convergence of Fourier series.
There are many different criteria and types of convergence for Fourier series;
here we use the following version.
\begin{theorem}
	\label{theorem:Fourier-converge}
	Let $f$ be continuous and periodic on $[0, 1]$, and
	\begin{equation}
		\sum_{k\in\bbZ}\abs{\hat{f}(k)}<\infty,
	\end{equation}
	then for all $x\in\bbR$,
	\begin{equation}
		f(x)=\sum_{k\in\bbZ}\hat{f}(k)e^{2\pi ikx}
	\end{equation}
\end{theorem}
Finally, the convergence theorem above uses the fact that Fourier series is unique.
\begin{theorem}
	\label{theorem:Fourier-unique}
	Suppose $f$ is continuous and periodic on $[0, 1]$, and satisfies $\int f<\infty$ and also $\hat{f}(k)=0$ for all $k\in\bbZ$, then $f=0$.
\end{theorem}

We additionally need a theorem on sufficient conditions for deriving infinite sums.
\begin{theorem}
	\label{theorem:derive-infinite-sum}
	Let $f_k:\bbR\rightarrow\bbR$ where $f_k\rightarrow f$ pointwise and $f'_k\rightarrow g$ uniformly for some $f, g: \bbR\rightarrow\bbR$.
	Assume further that everything is $C^1$.
	We have $f'=g$.
\end{theorem}

\section{Proofs of the lemmas}

We present the proofs in roughly reverse order, so that no proof uses anything unproven.
We omit the proof for \Cref{theorem:exchange-sum-and-integral} since it's clearly true for Riemann integrals.
First, \Cref{theorem:Fourier-unique} and \Cref{theorem:Fourier-converge} are standard material found in textbooks such as \cite{fourier-textbook}.
We include their proofs here for completeness.

\begin{proof}[Proof of \Cref{theorem:Fourier-unique}]
	We assume without loss of generality that $f$ is defined on $[-\pi, \pi]$.
	Suppose for the sake of contradiction that $f(0)>0$; this choice of nonzero point is also without loss of generality since Fourier series have properties for horizontal shifts.
	Since $f$ is continuous then, there exists some neighborhood $\delta<\frac{\pi}{2}$ so that $f(x)>\frac{y}{2}$ for all $x\in(x_0 - \delta, x_0 + \delta)$.
	We next define
	\begin{equation}
		p(\theta)=\varepsilon+\cos\theta
	\end{equation}
	where
	$$\varepsilon=\frac{1}{4}\parens{1-\cos\delta}$$
	is chosen so for all $\delta\leq\abs{\theta}\leq\pi$ we have $\abs{p\parens{\theta}}<1-\frac{\varepsilon}{2}$.
	We next choose $0<\eta<\delta$ so that $p(\eta)\geq 1 +\frac{\varepsilon}{2}$ which we can do due to continuity of $p$.
	Notice that
	$$p_k\parens{\theta}=\sqparens{p\parens{\theta}}^k$$
	is a trigonometric polynomial, so we have
	\begin{equation*}
		\int_{-\pi}^\pi f(\theta) p_k(\theta)d\theta = 0.
	\end{equation*}
	We however see that can't be true, since as $k\rightarrow\infty$ we have
	\begin{equation*}
		\int_{\abs{\theta}<\eta}f(\theta)p_k(\theta)\geq 2\eta\frac{f(0)}{2}\parens{1+\frac{\varepsilon}{2}}^k\rightarrow\infty
	\end{equation*}
	while
	\begin{equation*}
		\int_{\eta\leq\abs{\theta}<\delta}f(\theta)p_k(\theta)\geq 0
	\end{equation*}
	and the rest of the integral is finite for continuous (and therefore bounded) $f$.
\end{proof}

\begin{proof}[Proof of \Cref{theorem:Fourier-converge}]
	We have that
	\begin{equation}
		\sum_{k=-n}^n \hat{f}(k) e^{2\pi i k x}
	\end{equation}
	converges absolutely and uniformly.
	We now define
	\begin{equation}
		g(x)=\lim_{n\rightarrow\infty}\sum_{k=-n}^n \hat{f}(k) e^{2\pi i k x}.
	\end{equation}
	Observe that $g(x)$ is continuous, as it is the limit of a sequence of continuous functions that converges uniformly.
	The uniform convergence also allows us to use \Cref{theorem:exchange-sum-and-integral} to show that $\hat{g}(k)=\hat{f}(k)$.
	Applying \Cref{theorem:Fourier-unique} to $f-g$ then gives $f=g$.
\end{proof}

We next prove the Poisson summation formula.

\begin{proof}[Proof of \Cref{lemma:Poisson-summation}]
	Define
	\begin{equation}
		F(x)=\sum_{n\in\bbZ}f(x+n)
	\end{equation}
	which is well-defined as $f$ is Schwarz.
	Observe that $F$ is periodic and we can write down its Fourier coefficients:
	\begin{align}
		\hat{F}(k)&=\int_0^1 \sum_{n\in\bbZ} f(x+n)e^{-2\pi ikx} \\
		&=\sum_{n\in\bbZ} \int_0^1 f(x+n)e^{-2\pi ikx} \\
		&=\sum_{n\in\bbZ} \int_n^{n+1} f(x)e^{-2\pi ikx} \\
		&=\int_{-\infty}^{\infty} f(x)e^{-2\pi ikx} \\
		&=\hat{f}(k)
	\end{align}
	where the switch between sum and integral is again due to \Cref{theorem:exchange-sum-and-integral}.
	By applying \Cref{theorem:Fourier-converge} then, we have
	\begin{equation}
		F(x)=\sum_k \hat{f}(k) e^{ikx}.
	\end{equation}
	The conclusion follows by substituting $x=0$.
\end{proof}

We next take the proofs of \Cref{lemma:DG-renyi-bound} and \Cref{lemma:norm-shifted-DG} from \cite{discrete-gaussian}.

\begin{proof}[Proof of \Cref{lemma:norm-shifted-DG}]
	Let $f(x)=e^{-x^2/2\sigma^2}$ and we have its Fourier coefficient
	\begin{equation}
		\hat{f}(y)=\sqrt{2\pi\sigma^2}e^{-2\pi^2\sigma^2y^2}.
	\end{equation}
	By applying \Cref{lemma:Poisson-summation} to $g(x, t)=e^{-(x+t)^2/2\sigma^2}$,
	we have
	\begin{equation}
		\sum_{x\in\bbZ}f(x+t)=\sum_{y\in\bbZ}\hat{f}(y)e^{2\pi iyt}.
	\end{equation}
	We then have
	\begin{align}
		\sum_{x\in\bbZ}e^{-(x-\mu)/2\sigma^2}
		&=\sum_{x\in\bbZ}f(x-\mu)\\
		&=\abs{\sum_{x\in\bbZ}f(x-\mu)} \\
		&=\abs{\sum_{y\in\bbZ}\hat{f}(y)e^{-2\pi iy\mu}} \\
		&\leq\sum_{y\in\bbZ}\abs{\hat{f}(y)e^{-2\pi iy\mu}} \\
		&=\sum_{y\in\bbZ}\hat{f}(y) \\
		&=\sum_{x\in\bbZ}f(x) \\
		&=\sum_{x\in\bbZ}e^{-x/2\sigma^2}
	\end{align}
\end{proof}

\begin{proof}[Proof of \Cref{lemma:DG-renyi-bound}]
	We take $\nu=0$ without loss of generality.
	We have
	\begin{align}
		&e^{(\alpha-1)\renyi{\DG{\mu}{\sigma^2}}{\DG{0}{\sigma^2}}}\\
		&\qquad=\sum_{x\in\bbZ}\parens{\Pr\sqparens{\DG{\mu}{\sigma^2}=x}}^\alpha\parens{\Pr\sqparens{\DG{0}{\sigma^2}=x}}^{1-\alpha}\\
		&\qquad=\sum_{x\in\bbZ}
			\parens{
				\frac{e^{-(x-\mu)^2/2\sigma^2}}{\sum_y e^{-(y-\mu)^2/2\sigma^2}}
			}^\alpha
			\parens{
				\frac{e^{-x^2/2\sigma^2}}{\sum_y e^{-y^2/2\sigma^2}}
			}^{1-\alpha}\\
		&\qquad=\frac{\sum_x e^{\parens{-x^2+2\alpha\mu x-\alpha\mu^2}/2\sigma^2}}{\sum_y e^{-y^2/2\sigma^2}}\\
		&\qquad=e^{\alpha(\alpha-1)}\cdot\frac{\sum_x e^{-\parens{x-\alpha\mu}^2/2\sigma^2}}{\sum_y e^{-y^2/2\sigma^2}}\\
		&\qquad\leq e^{\alpha(\alpha-1)}
	\end{align}
	where the final inequality is due to \Cref{lemma:norm-shifted-DG}.
\end{proof}

Before finishing the proof, we need a fact on derivatives of infinite sum.
\begin{proof}[Proof of \Cref{theorem:derive-infinite-sum}]
	$\int f_k'\rightarrow \int g$ due to uniform convergence assumption.
	We also have $\int f_k'=f_k\rightarrow f$.
	Due to uniqueness of limits and the $C^1$ condition we have $f=\int g$.
	Taking derivatives on both sides finishes the proof.
\end{proof}

We finally prove \Cref{lemma:KL-renyi} and observe that \Cref{theorem:noise-flooding} follows directly from \Cref{lemma:KL-renyi} and \Cref{lemma:DG-renyi-bound}.

\begin{proof}[Proof of \Cref{lemma:KL-renyi}]
	Notice that
	\begin{equation}
		\lim_{\alpha\rightarrow 1}\frac{f(\alpha)}{\alpha-1}=f'(1)
	\end{equation}
	if the derivative exists.
	We therefore try to take the derivative
	\begin{align*}
		\frac{d}{d\alpha}
		&\ln\parens{\sum_{x\in\bbZ}
		\parens{\Pr\sqparens{\cP=x}}^\alpha
		\parens{\Pr\sqparens{\cQ=x}}^{1-\alpha}} \\
		&\qquad=
		\frac{
			\frac{d}{d\alpha}
			\sum_{x\in\bbZ}
			\parens{\Pr\sqparens{\cP=x}}^\alpha
			\parens{\Pr\sqparens{\cQ=x}}^{1-\alpha}
		}{
			\sum_{x\in\bbZ}
			\parens{\Pr\sqparens{\cP=x}}^\alpha
			\parens{\Pr\sqparens{\cQ=x}}^{1-\alpha}
		}
	\end{align*}
	We next exchange the derivative and the infinite sum using \Cref{theorem:derive-infinite-sum},
	for which all the assumptions are satisfied by discrete Gaussian density functions.
	(Here we can use monotone convergence instead, but I don't know if that's any simpler to prove.)
	We then take the derivative
	\begin{align*}
		\frac{d}{d\alpha}&
		\parens{\Pr\sqparens{\cP=x}}^\alpha
		\parens{\Pr\sqparens{\cQ=x}}^{1-\alpha} \\
		&\qquad=\parens{\Pr\sqparens{\cP=x}}^\alpha
		\parens{\Pr\sqparens{\cQ=x}}^{1-\alpha}
		\ln\parens{\frac{\Pr\sqparens{\cP=x}}{\Pr\sqparens{\cQ=x}}}
	\end{align*}
	We arrive at the conclusion by collecting everything so far and substituting $\alpha=1$.
\end{proof}

\subsection{A Tail Bound for Discrete Gaussian}

Using the machinery developed so far,
we can also easily show a tail-bound for the discrete Gaussian.
One simply shows that discrete Gaussian is a \emph{subgaussian} distribution,
then use the tail bounds for subgaussian distributions.
We begin by defining what are subgaussian distributions.
\begin{definition}[subgaussian distributions]
	A random variable $X$ over reals is \emph{subgaussian} if its moment generating function satisfies the \emph{Laplace transform condition}:
	there exists $b>0$ so that for all $t\in\bbR$,
	\begin{equation}
		\E\sqparens{e^{tX}} \leq e^{b^2t^2/2}
	\end{equation}
\end{definition}

We show that the discrete Gaussian is indeed subgaussian.
\begin{lemma}
	The discrete Gaussian distribution $\DG{0}{\sigma^2}$ is subgaussian with $b^2=\sigma^2$.
\end{lemma}
\begin{proof}
	By \Cref{lemma:norm-shifted-DG}, we have
	\begin{align}
		\E\sqparens{e^{tX}}
		&=\frac{\sum_x e^{tx-x^2/2\sigma^2}}{\sum_y e^{-y^2/2\sigma^2}} \\
		&=\frac{\sum_x e^{-\parens{x-t\sigma^2}^2/2\sigma^2}\cdot e^{t^2\sigma^2/2}}{\sum_y e^{-y^2/2\sigma^2}} \\
		&\leq e^{t^2\sigma^2/2}
	\end{align}
\end{proof}

Next we take the proof from \cite{subgaussian} that tails of subgaussian random variables decay at least as fast as Gaussians.
\begin{lemma}
	Suppose $X$ is subgaussian, then there exists $c>0$ so that for all $\lambda>0$,
	\begin{equation}
		\Pr\sqparens{\abs{X}\geq\lambda}\leq 2e^{-c\lambda^2}
	\end{equation}
\end{lemma}
\begin{proof}
	Using Markov's inequality, we have
	\begin{align}
		\Pr\sqparens{X\geq\lambda}&=\Pr\sqparens{tX\geq t\lambda} \\
		&\leq \frac{\E\sqparens{e^{tX}}}{e^{tX}} \\
		&\leq e^{-t\lambda + b^2 t^2 /2}
	\end{align}
	Choosing $t=\frac{\lambda}{2b^2}$ and doing the same calculation for the left tail concludes the proof.
\end{proof}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
